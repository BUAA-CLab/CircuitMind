# TC-Bench 模型评估流程

## 简介

本目录包含一系列脚本，用于自动化评估基于 TC-Bench 基准测试集的电路生成模型。评估流程主要包括：

1.  **功能正确性验证:** 通过运行 TC-Bench 提供的测试平台（testbench）来检查模型生成的电路设计的逻辑功能是否正确。
2.  **性能指标提取:** 从成功的仿真中提取关键性能指标，如逻辑门数量（Gate Count）和关键路径延迟（Delay）。
3.  **多样性分析:** 衡量模型为每个挑战生成的不同解决方案的数量。
4.  **通过率计算:** 计算 Pass@k 指标（如 Pass@1, Pass@5），评估模型在有限尝试内生成正确方案的概率。
5.  **结果汇总与比较:** 生成汇总表格和对比图表，方便比较不同模型的性能。

## 前提条件 (Prerequisites)

在运行评估脚本之前，请确保满足以下条件：

**1. 软件依赖:**

* Bash Shell 环境
* Python 3.x (建议 3.7 或更高版本)
* [Icarus Verilog (`iverilog`)](http://iverilog.icarus.com/): 用于 Verilog 代码的编译和仿真。请确保 `iverilog` 和 `vvp` 命令在您的系统路径中可用。
* [jq](https://stedolan.github.io/jq/): 一个命令行 JSON 处理工具（`run_single.sh` 脚本需要）。

**2. Python 库依赖:**

* 运行以下命令安装所需的 Python 包：
    ```bash
    pip install -r requirements.txt
    ```

**3. TC-Bench 基准测试集:**

* 您需要拥有完整的 TC-Bench 数据集，特别是每个挑战对应的测试平台文件（通常是 `testbench.v` 或类似名称）。
* 评估流程依赖于一个用户提供的 `run.sh` 脚本（详见下文），该脚本需要能够找到并使用正确的测试平台文件来运行仿真。请确保 `run.sh` 知道测试平台的相对或绝对路径。

**4. 模型输出数据结构:**

* **重要:** 您的模型生成的 Verilog 电路设计文件**必须**按照以下目录结构进行组织：

    ```
    <模型结果基础目录>/                # 例如: ../cleaned-exp/MyCoolModel (对应 $MODEL_SRC_DIR)
    ├── <挑战名称_1>/                 # 例如: 1_not_gate
    │   ├── <尝试编号_1>/             # 例如: 1
    │   │   ├── your_model_output.v # 模型生成的 Verilog 文件 (.v)
    │   │   └── run.sh              # **必需**: 用于运行本次尝试仿真的脚本
    │   ├── <尝试编号_2>/             # 例如: 2
    │   │   ├── your_model_output.v
    │   │   └── run.sh
    │   └── ...                     # 例如: ... 直到 N (如 20)
    ├── <挑战名称_2>/                 # 例如: 3_xor_gate
    │   ├── <尝试编号_1>/             # 例如: 1
    │   │   ├── another_output.v
    │   │   └── run.sh
    │   └── ...
    └── ...                         # 其他所有 TC-Bench 挑战
    ```

    * `<模型结果基础目录>`: 存放特定模型所有输出的根目录。
    * `<挑战名称>`: 必须与 TC-Bench 的挑战目录名称完全一致（如 `1_not_gate`, `23_logic_engine`）。
    * `<尝试编号>`: **必须是纯数字**（如 `1`, `2`, ..., `20`），代表对该挑战的第 N 次生成/解决尝试。评估脚本会遍历这些数字目录。


## TC-Bench Evaluation Workflow Explanation

This document explains the sequence of scripts used to evaluate circuit generation models against the TC-Bench benchmark. The workflow involves simulating generated circuits, extracting performance metrics, analyzing results, and comparing models.

You can run each script individually for debugging or understanding, or use the main orchestration script (`evaluate.sh`) to run the entire process for a single model automatically.

---

### Individual Script Descriptions (English)

1.  **Single Attempt Script (`run.sh` located *inside* each attempt folder)**
    * **Purpose:** Provided by the user. This script is responsible for running the simulation for a *single* generated circuit attempt against the corresponding TC-Bench testbench. It compiles the Verilog code, runs the simulation (e.g., using `iverilog` and `vvp`), and analyzes the results.
    * **Inputs:**
        * The Verilog (`.v`) file(s) generated by the model for this specific attempt (located in the same directory).
        * Needs access to the relevant TC-Bench `testbench.v` (path logic is internal to this script).
    * **Outputs:**
        * Prints performance metrics to **standard output** in a specific format required by `extract-v2.py`, for example:
            ```
            Longest delay: [float] ns
            Longest path: [string]
            Total logic gates: [integer]
            Total delay: [integer]
            ```
        * May print simulation success/failure messages.
        * Should use exit code 0 for success, non-zero for failure.

2.  **`run_single.sh`**
    * **Purpose:** To find the best result (minimum gate count) for *one specific challenge* (e.g., `3_xor_gate`) across all its attempts and record this minimum value. Useful for optimizing or tracking specific base components.
    * **Inputs:**
        * `<challenge_folder_path>`: Path to the specific challenge directory containing attempt subfolders (e.g., `../cleaned-exp/MyModel/3_xor_gate`).
        * `<script_name>`: Name of the script to run inside each attempt folder (usually `"run.sh"`).
        * `<input_json_file>`: Path to a template JSON file.
        * `<output_json_file>`: Path where the updated JSON file (with the min gate count) will be saved.
        * `<temp_results_file>`: Path for a temporary file to store intermediate gate counts.
    * **Outputs:**
        * Creates/updates the `<output_json_file>` with the minimum gate count found.
        * Prints progress to standard output.
        * Creates and potentially deletes the `<temp_results_file>`.

3.  **Batch Simulation Script (`run-v2.sh`, or potentially renamed e.g., `run.sh` in the evaluation directory)**
    * **Purpose:** To automatically execute the *Single Attempt Script* (`run.sh`) for **all** attempts across **multiple** (or all) challenge folders for a given model. It orchestrates the bulk simulation process.
    * **Inputs:**
        * `<model_results_base_dir>`: Path to the base directory containing all challenge subfolders for the model.
        * `[optional_challenge_names...]`: Optional list of specific challenge folder names to process (less commonly used in the current setup).
    * **Outputs:**
        * Prints detailed execution progress for each challenge and attempt to **standard output**. This includes the crucial `--> Executing 'run.sh' in '...'` lines.
        * The standard output of this script **is intended to be captured** into a main log file (e.g., using `tee`). This log file becomes the primary input for the extraction script.
        * Prints warnings (e.g., long execution time) and errors to standard output/error.

4.  **`Verilog_Diversity_Test.py`**
    * **Purpose:** To measure the diversity of solutions generated by the model for each challenge by counting the number of unique Verilog file contents.
    * **Inputs:**
        * `--input-dir`: Path to the base directory containing all challenge subfolders for the model.
    * **Outputs:**
        * Prints a formatted table (`PrettyTable`) to **standard output** showing the diversity count per challenge. This output is typically captured by `tee`.

5.  **`extract-v2.py` (or `extract.py`)**
    * **Purpose:** To parse the main log file generated by the *Batch Simulation Script* (step 3), extract the performance metrics for each successful attempt, identify failed attempts, and save the structured data.
    * **Inputs:**
        * `--log-file`: Path to the main log file captured from the standard output of the Batch Simulation Script.
        * `--output-csv`: Path where the extracted raw results will be saved in CSV format (`*-raw.csv`).
    * **Outputs:**
        * Creates the `*-raw.csv` file containing extracted metrics per attempt, with challenge separators and empty metrics for failed runs.
        * Prints progress, status (e.g., "Data successfully saved..."), and a list of detected failed simulations to standard output.

6.  **`analysis_gate-v2.py` (or `analysis_gate.py`)**
    * **Purpose:** To analyze the raw results CSV, calculate a combined metric (gates + delay), filter out invalid/zero results, find the single best-performing attempt for each challenge based on the minimum combined metric, and save these best results.
    * **Inputs:**
        * `--input-csv`: Path to the raw results CSV file (`*-raw.csv`) generated by `extract-v2.py`.
        * `--output-csv`: Path where the filtered "best results" will be saved in CSV format (`*-less.csv`).
    * **Outputs:**
        * Creates the `*-less.csv` file containing only the best result row for each challenge.
        * Prints status messages to standard output.

7.  **`pass_ratio-v2.py` (or `pass_ratio.py`)**
    * **Purpose:** To calculate the pass@k success rates (e.g., pass@1, pass@5) for each challenge based on the number of successful attempts recorded in the raw results CSV, assuming a fixed total number of attempts per challenge.
    * **Inputs:**
        * `--input-csv`: Path to the raw results CSV file (`*-raw.csv`).
        * `--output-results-csv`: Path to save the calculated pass rate results in CSV format.
        * `--output-plot-pass1`: Path to save the pass@1 bar chart PNG.
        * `--output-plot-pass5`: Path to save the pass@5 bar chart PNG.
        * `--total-trials`: The total number of attempts assumed per challenge (integer, default: 20).
    * **Outputs:**
        * Creates the pass rate results CSV file.
        * Creates the pass@1 and pass@5 PNG plot files.
        * Prints a summary table (`PrettyTable`) of the pass rates to standard output (typically captured by `tee`).

8.  **`merge_gates-v2.py` (or `merge_gates.py`)**
    * **Purpose:** To combine the best gate count results (`Total logic gates` from `*-less.csv` files) from **multiple models** into a single summary table for easy comparison.
    * **Inputs:**
        * `--input-dir`: Path to the directory containing the result folders for **all** models to be compared (e.g., `./exp-data-v0/ModelTest/`). The script searches recursively for `*-less.csv` files within this directory.
        * `--output-csv`: Path where the merged comparison table will be saved in CSV format.
    * **Outputs:**
        * Creates the merged CSV file (e.g., `GATES-merged-*.csv`).
        * Prints status messages to standard output.

9.  **`model_res_compare_gates-v2.py` (or `model_res_compare_gates.py`)**
    * **Purpose:** To generate a scatter plot comparing the best gate counts (`Total logic gates` from `*-less.csv` files) achieved by **multiple models** across all challenges.
    * **Inputs:**
        * `--input-dir`: Path to the directory containing the result folders for **all** models to be compared (e.g., `./exp-data-v0/ModelTest/`). The script searches recursively for `*-less.csv` files.
        * `--output-png`: Path where the comparison plot PNG file will be saved.
    * **Outputs:**
        * Creates the comparison plot PNG file (e.g., `GATES-compared-*.png`).
        * Prints status messages to standard output.

---

### Orchestration Script (`evaluate.sh`)

* **Purpose:** This master script automates the entire evaluation workflow for a **single model**. It sets up necessary directories and calls the individual scripts described above (steps 2 through 7) in the correct sequence, passing the appropriate inputs and outputs between them. It handles the configuration for a specific model run.
* **Inputs:**
    * Configuration variables set within the script itself (e.g., `MODEL_NAME`, `MODEL_SRC_BASE_DIR`, `OUTPUT_BASE_DIR`, `TOTAL_TRIALS`, etc.).
* **Outputs:**
    * Orchestrates the creation of all the output files and directories described for scripts 2-7 within the specified `OUTPUT_BASE_DIR`.
    * Prints overall progress messages to standard output.

---

### Running the Workflow

There are two primary ways to execute this evaluation:

1.  **Manual Execution (Step-by-Step):** You can run each script (`run_single.sh`, the Batch Simulation Script, `Verilog_Diversity_Test.py`, `extract-v2.py`, etc.) individually from the command line, providing the required arguments. This is useful for debugging specific parts of the process or if you only need a subset of the analysis. Ensure the output of one step is available as input for the next. Remember to capture the output of the Batch Simulation script into a log file for `extract-v2.py`.

2.  **Automated Execution (Using `evaluate.sh`):** This is the recommended method for standard evaluation runs.
    * Configure the variables at the top of the `evaluate.sh` script (model name, paths, etc.).
    * Ensure all individual scripts (`run_single.sh`, the Batch Simulation Script, all `.py` scripts) are present and executable (`chmod +x ...`).
    * Run the script using `bash evaluate.sh`.
    * This will execute steps 2 through 7 automatically for the configured model.

**Cross-Model Comparison:**

The scripts for merging results (`merge_gates-v2.py`) and generating comparison plots (`model_res_compare_gates-v2.py`) are designed to work across *multiple* models. They are typically **run separately** *after* you have executed `evaluate.sh` (or the individual steps) for each model you want to compare. Ensure the `--input-dir` argument for these scripts points to the parent directory containing all the individual model result folders (e.g., `./exp-data-v0/ModelTest/`).

---
