Large language models (LLMs) have transformed code generation, yet their application in hardware design produces gate counts 38\--1075\ higher than human designs. We present CircuitMind, a multi-agent framework that achieves human-competitive efficiency through three key innovations: syntax locking (constraining generation to basic logic gates), retrieval-augmented generation (enabling knowledge-driven design), and dual-reward optimization (balancing correctness with efficiency). To evaluate our approach, we introduce TC-Bench, the first gate-level benchmark harnessing collective intelligence from the TuringComplete ecosystem---a competitive circuit design platform with hundreds of thousands of players. Experiments show CircuitMind enables 55.6\ of model implementations to match or exceed top-tier human experts in composite efficiency metrics. Notably, our framework elevates the 14B-parameter Phi-4 model (ranked 6th among 1,000 human experts on TuringComplete) to outperform both GPT-4o mini and Gemini 2.0 Flash without requiring specialized training. These innovations establish a new paradigm for hardware optimization where collaborative AI systems leverage collective human expertise to achieve optimal circuit designs. Our model, data, and code are open-source at https://github.com/BUAA-CLab/CircuitMind.